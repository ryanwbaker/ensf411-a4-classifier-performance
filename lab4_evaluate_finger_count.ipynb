{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Assessment strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: State which assessment strategy you have chosen and explain how you have implemented it.\n",
    "\n",
    "*Selected Option 2: Print labels to standard out.*\n",
    "\n",
    "*Implemented by adding an extra command line argument --finger, where the actual finger can be entered (one, two, three, four, or five). From there, this argument is parsed and passed into the FingerCountFrame __init__ function, and printed to stdout when the prediction is made. Additionally, this was printed to a .csv file for easy data collection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyze results from production prototype\n",
    "\n",
    "At least 5 examples for each digit should be available and stored in `lab4-results.csv` and compute:\n",
    "- accuracy\n",
    "- confusion matrix \n",
    "\n",
    "here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[24  5  7  4  1]\n",
      " [ 0 26  0 10  0]\n",
      " [ 0  1 31  0  0]\n",
      " [ 0 23  0 21  0]\n",
      " [ 0 10 23  0 27]] \n",
      "\n",
      "Accuracy Score:\n",
      " 0.6056338028169014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lab4-results.csv', header=None, names=['actual', 'pred', 'prob'])\n",
    "\n",
    "print('Confusion Matrix:\\n', confusion_matrix(df['actual'], df['pred']),'\\n')\n",
    "\n",
    "print('Accuracy Score:\\n',accuracy_score(df['actual'], df['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: How many predictions did you analyse and how did you select the samples?\n",
    "\n",
    "*213 predictions were analyzed and samples were selected as raw output from the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summary and Conclusion\n",
    "Summarize:\n",
    "- Results from production analysis\n",
    "- Re-state training, validation results from Lab3\n",
    "\n",
    "*Results from production analysis: 0.605*\n",
    "*Training loss results from lab 3: 0.45*\n",
    "*Validation accuracy from lab 3: 0.48*\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "- Does validation accuracy translate to the production environment?\n",
    "- Are you satisfied with model performance? Why, why not?\n",
    "- Provide directions for future work, e.g. how could your model and task be improved?\n",
    "\n",
    "*Validation accuracy appears to be higher in the production environment. This may be due to \"average\" photos of the fingers being used, whereas in the training data, more extreme photos were used as an attempt to better train the model (eg. close up, far away, weird orientations, etc)*\n",
    "\n",
    "*For a production model, this is still unsatisfactory. The model is only getting it right 2/3 of the time. In using deep learning for any type of practical model, it's hard to imagine a scenario where this would be an acceptable amount of error*\n",
    "\n",
    "*Future work should try with a larger and more extensive data set to see if the model learns better.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reflection\n",
    "Include a sentence or two about\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "\n",
    "while working on this assignment.\n",
    "\n",
    "*I enjoyed the vagueness of this assignment. It felt like doing something in the real world, with no instruction or direction; just requirements to complete.*\n",
    "\n",
    "*I spent a significant amount of time deciding on the best option to extract the results, but overall I'm happy with choosing option 2; it seemed very efficient.*\n",
    "\n",
    "*I was slightly confused about using the confusion matrix outside of FastAI (or if this was acceptable), but given the context of the assignment, I assumed that any way the results were achieved would be acceptable. I chose the most efficient method I knew of.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
